{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark: Koalas (PySpark) and Dask - Data Preparation\n",
    "The benchmark was performed against the 2009 - 2013 Yellow Taxi Trip Records (157 GB) from NYC Taxi and Limousine Commission (TLC) Trip Record Data.\n",
    "\n",
    "The CSV files were downloaded into Databricks File System (DBFS), and then were converted into Parquet files via Koalas for better efficiency.\n",
    "\n",
    "Download url: https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page.\n",
    "\n",
    "Data dictionary: https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf.\n",
    "\n",
    "The scenario used in this benchmark was inspired by https://github.com/xdssio/big_data_benchmarks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download CSV files to DBFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'sh'\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "sudo mkdir -p ../../datasets/taxi_csv/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'sh'\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "ls ../../datasets/taxi_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The old download version didn't work, changed it to a new one using \"download_taxi_datasets.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url_loc = {} # Map download url to the file location in DBFS\n",
    "\n",
    "# for year in range(2009, 2014):\n",
    "#   for m in range(1, 13):\n",
    "#     month = \"{:02d}\".format(m)\n",
    "#     fname = 'yellow_tripdata_%s-%s.csv' % (year, month)\n",
    "#     url = 'https://s3.amazonaws.com/nyc-tlc/trip+data/%s' % fname\n",
    "#     loc = f'{DATASETS_DIR}/taxi_csv/%s' % fname\n",
    "#     url_loc[url] = loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import urllib.request\n",
    "\n",
    "# for url, loc in url_loc.items():\n",
    "#   urllib.request.urlretrieve(url, loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our version to download data\n",
    "\n",
    "<span style=\"font-size:20pt\"><b>NOTE:</b> </span> the dataset changed so we had to change the years of this analysis... Initialy this was between 2009 and 2014, now it's from 2011 until 2015... (The columns changed from 2011 forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:02<00:00,  8.83it/s]\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "import os\n",
    "\n",
    "# Add \"code\" folder to path so we can import the \"download_taxi_files\" script\n",
    "import sys\n",
    "sys.path.insert(1, os.path.abspath('..'))\n",
    "from download_taxi_datasets import download_taxi_files    \n",
    "\n",
    "DATASETS_DIR = '../../datasets'\n",
    "years = range(2011, 2013)\n",
    "date_range = [dt.datetime(year, month, 1) for year in years for month in range(1, 13)]\n",
    "files = download_taxi_files(date_range, f'{DATASETS_DIR}/taxi_csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'sh'\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "ls ../../datasets/taxi_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.392001261 GBs data in total\n"
     ]
    }
   ],
   "source": [
    "# Changed bellow section to work in a normal Linux machine\n",
    "# total_bytes = 0\n",
    "# for fileInfo in dbutils.fs.ls(f'{OUTPUT_FOLDER}/taxi_csv'):\n",
    "#   total_bytes += fileInfo.size\n",
    "import os\n",
    "total_bytes = 0\n",
    "for filename in os.listdir(f'{DATASETS_DIR}/taxi_csv'):\n",
    "    file_path = os.path.join(f'{DATASETS_DIR}/taxi_csv', filename)\n",
    "    if os.path.isfile(file_path): total_bytes += os.path.getsize(file_path)\n",
    "print('%s GBs data in total' % (total_bytes * 1e-9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to Parquet files\n",
    "Convert downloaded CSV files into Parquet files via Koalas for better efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"border: 2px solid white;\">\n",
       "<tr>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Client</h3>\n",
       "<ul style=\"text-align: left; list-style: none; margin: 0; padding: 0;\">\n",
       "  <li><b>Scheduler: </b>tcp://127.0.0.1:63370</li>\n",
       "  <li><b>Dashboard: </b><a href='http://127.0.0.1:8787/status' target='_blank'>http://127.0.0.1:8787/status</a></li>\n",
       "</ul>\n",
       "</td>\n",
       "<td style=\"vertical-align: top; border: 0px solid white\">\n",
       "<h3 style=\"text-align: left;\">Cluster</h3>\n",
       "<ul style=\"text-align: left; list-style:none; margin: 0; padding: 0;\">\n",
       "  <li><b>Workers: </b>1</li>\n",
       "  <li><b>Cores: </b>2</li>\n",
       "  <li><b>Memory: </b>21.47 GB</li>\n",
       "</ul>\n",
       "</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<Client: 'tcp://127.0.0.1:63370' processes=1 threads=2, memory=21.47 GB>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import databricks.koalas as ks\n",
    " \n",
    "# ks.set_option('compute.default_index_type', 'distributed-sequence')\n",
    "\n",
    "# Instead of using koalas to rename the columns, we're going to use dask\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client, LocalCluster\n",
    "\n",
    "# cluster = LocalCluster(n_workers=4, threads_per_worker=3, memory_limit='30GiB')\n",
    "cluster = LocalCluster(n_workers=1, threads_per_worker=2, memory_limit='20GiB')\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing columns names\n",
    "\n",
    "In order for the datasets to match the format of the ones in the article, we had to change some of the column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "renaming_cols = {\n",
    "  'passenger_count': 'Passenger_Count',\n",
    "  'fare_amount': 'Fare_Amt',\n",
    "  'tip_amount': 'Tip_Amt',\n",
    "  # The 2 renamings bellow, don't show the exact LAT/LON, \n",
    "  # but we only want to benchmark, so they're not entirely relevant...\n",
    "  # We'll later create the respective Lat coordinates by copying the values of this lon ones\n",
    "  'PULocationID': 'Start_Lon',\n",
    "  'DOLocationID': 'End_Lon',\n",
    "\n",
    "  'tolls_amount': 'Tolls_Amt',\n",
    "  'total_amount': 'Total_Amt',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype_dict = {\n",
    "  'Passenger_Count': 'int64', \n",
    "  'Start_Lon': 'float64', \n",
    "  'Start_Lat': 'float64',\n",
    "  'End_Lon': 'float64', \n",
    "  'End_Lat': 'float64', \n",
    "  'Fare_Amt': 'float64', \n",
    "  'Tip_Amt': 'float64', \n",
    "  'Tolls_Amt': 'float64',\n",
    "  'Total_Amt': 'float64'\n",
    "}\n",
    "# Convert Article dtype_dict to new one\n",
    "dtype_dict = {col: dtype_dict[renaming_cols[col]] for col in renaming_cols.keys()}\n",
    "ks_df = dd.read_parquet(f'{DATASETS_DIR}/taxi_csv', dtype=dtype_dict)\n",
    "\n",
    "# Drop all other columns not relevant (to us)\n",
    "ks_df = ks_df.drop(columns=ks_df.columns.difference(dtype_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renaming columns to match old format\n",
    "ks_df = ks_df.rename(columns=renaming_cols)\n",
    "\n",
    "# Create the Lat/Lon columns\n",
    "ks_df['Start_Lat'] = ks_df['Start_Lon'].copy()\n",
    "ks_df['End_Lat'] = ks_df['End_Lon'].copy()\n",
    "\n",
    "ks_df.columns = ks_df.columns.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Couldn't find program: 'sh'\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "rm -fr ../../datasets/ks_taxi_parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ks_df.to_parquet(f'{DATASETS_DIR}/ks_taxi_parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.179561619 GBs data in total\n"
     ]
    }
   ],
   "source": [
    "# total_bytes = 0\n",
    "# for file_info in dbutils.fs.ls('FileStore/ks_taxi_parquet'):\n",
    "#   total_bytes += file_info.size\n",
    "# print('%s GBs data in total' % (total_bytes * 1e-9))\n",
    "\n",
    "total_bytes = 0\n",
    "for filename in os.listdir(f'{DATASETS_DIR}/ks_taxi_parquet'):\n",
    "    file_path = os.path.join(f'{DATASETS_DIR}/ks_taxi_parquet', filename)\n",
    "    if os.path.isfile(file_path): total_bytes += os.path.getsize(file_path)\n",
    "print('%s GBs data in total' % (total_bytes * 1e-9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note: Filtering Size\n",
    "(Size of filtered data / Size of total data) in the benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import databricks.koalas as ks\n",
    "koalas_data = ks.read_parquet(f'{DATASETS_DIR}/ks_taxi_parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the benchmark, filtered data is 45.102692369637836% of total data\n"
     ]
    }
   ],
   "source": [
    "expr_filter = (koalas_data.tip_amt >= 1) & (koalas_data.tip_amt <= 5)\n",
    " \n",
    "print(f'In the benchmark, filtered data is {len(koalas_data[expr_filter]) / len(koalas_data) * 100}% of total data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
